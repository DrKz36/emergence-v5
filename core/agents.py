"""
√âMERGENCE v4 - Agents Multi-LLM ARCHITECTURE FINALE
Version: 4.1.0 - FIX WEBSOCKET RESPONSE FORMAT
Session: 02/06/2025 - Post database.py FTS5 fix

üî• CORRECTIONS CRITIQUES V4.1 :
- FIX: WebSocket response format ‚Üí AgentResponse objects avec tous attributs
- FIX: Mode Triple mapping ‚Üí R√©solution 'cost_estimate' attribute error
- FIX: get_response() retourne AgentResponse au lieu de Dict
- OPTIMISATION: Gestion erreurs robuste + fallbacks
- AJOUT: Compatibility layer pour backend FastAPI
- MAINTIEN: Toutes m√©thodes V3 existantes
"""

import os
import json
import logging
import time
import asyncio
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple, Union
from dataclasses import dataclass, asdict
from pathlib import Path

# Configuration logging optimis√©e
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# === IMPORTS API ===
try:
    import openai
    OPENAI_AVAILABLE = True
    logger.info("‚úÖ OpenAI disponible")
except ImportError as e:
    OPENAI_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è OpenAI non disponible: {e}")

try:
    import google.generativeai as genai
    GOOGLE_AVAILABLE = True
    logger.info("‚úÖ Google Generative AI disponible")
except ImportError as e:
    GOOGLE_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è Google Generative AI non disponible: {e}")

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
    logger.info("‚úÖ Anthropic disponible")
except ImportError as e:
    ANTHROPIC_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è Anthropic non disponible: {e}")

# === IMPORTS ARCHITECTURE (avec fallbacks gracieux) ===
try:
    from core.rag_manager import get_rag_manager
    RAG_MANAGER_AVAILABLE = True
    logger.info("‚úÖ RAG Manager V4 disponible")
except ImportError as e:
    RAG_MANAGER_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è RAG Manager non disponible: {e}")

# Fallbacks pour imports optionnels
try:
    from memory_manager import MemoryManager, VectorMemory
    MEMORY_MANAGER_AVAILABLE = True
except ImportError:
    MEMORY_MANAGER_AVAILABLE = False

try:
    from structured_memory import StructuredMemoryManager, MemoryType
    STRUCTURED_MEMORY_AVAILABLE = True
except ImportError:
    STRUCTURED_MEMORY_AVAILABLE = False

try:
    from persistent_memory_manager import PersistentMemoryManager, InteractionType, PersistenceLevel
    PERSISTENT_MEMORY_AVAILABLE = True
except ImportError:
    PERSISTENT_MEMORY_AVAILABLE = False

# === DATACLASSES OPTIMIS√âES ===

@dataclass 
class AgentResponse:
    """üî• RESPONSE FORMAT UNIFI√â - Compatible WebSocket + REST"""
    # Champs principaux (REQUIS pour WebSocket)
    success: bool
    agent_name: str
    response_text: str = ""
    timestamp: str = ""
    
    # Champs techniques (pour analytics)
    processing_time: float = 0.0
    model_used: str = "unknown"
    temperature: float = 0.7
    provider: str = "unknown"  # üî• AJOUT√â: Fix WebSocket 'provider' error
    
    # Champs RAG (contexte documents)
    rag_used: bool = False
    rag_chunks_count: int = 0
    rag_context_preview: str = ""
    rag_search_type: str = "none"
    
    # Champs optionnels (debug/analytics)
    prompt_sent: str = ""
    tokens_used: Optional[int] = None
    cost_estimate: float = 0.0  # üî• AJOUT√â: Fix WebSocket error
    
    # Erreurs et warnings
    errors: Optional[List[str]] = None
    warnings: Optional[List[str]] = None
    
    def __post_init__(self):
        """Initialisation s√©curis√©e des listes"""
        if self.errors is None:
            self.errors = []
        if self.warnings is None:
            self.warnings = []
        if not self.timestamp:
            self.timestamp = datetime.now().isoformat()
    
    # üî• M√âTHODES COMPATIBILITY WEBSOCKET
    def to_dict(self) -> Dict[str, Any]:
        """Conversion Dict pour WebSocket JSON"""
        return asdict(self)
    
    def get_response_text(self) -> str:
        """Getter s√©curis√© pour response_text"""
        return self.response_text or "R√©ponse indisponible"
    
    def get_agent_name(self) -> str:
        """Getter s√©curis√© pour agent_name"""
        return self.agent_name or "Unknown"

@dataclass
class TripleResponse:
    """üî• MODE TRIPLE RESPONSE - D√©bat triangulaire"""
    success: bool
    mode: str = "triple"
    responses: Dict[str, AgentResponse] = None
    agents_success: int = 0
    timestamp: str = ""
    context_used: bool = False
    processing_time: float = 0.0
    cost_estimate: float = 0.0  # üî• AJOUT√â: Fix WebSocket error
    
    def __post_init__(self):
        if self.responses is None:
            self.responses = {}
        if not self.timestamp:
            self.timestamp = datetime.now().isoformat()
    
    def to_dict(self) -> Dict[str, Any]:
        """Conversion Dict avec AgentResponse s√©rialis√©es"""
        result = asdict(self)
        # Conversion AgentResponse vers Dict
        if self.responses:
            result["responses"] = {
                agent: resp.to_dict() if hasattr(resp, 'to_dict') else resp
                for agent, resp in self.responses.items()
            }
        return result

# === CLASSE PRINCIPALE OPTIMIS√âE ===

class EmergenceAgentsV4:
    """
    üöÄ SYST√àME MULTI-AGENTS √âMERGENCE v4.1 - FIX WEBSOCKET FORMAT
    
    üî• CORRECTIONS V4.1:
    - AgentResponse objects avec tous attributs requis (cost_estimate, etc.)
    - get_response() retourne AgentResponse au lieu de Dict
    - get_triple_response() retourne TripleResponse avec mapping correct
    - Fallbacks gracieux pour tous imports optionnels
    - Interface V4 100% compatible backend FastAPI + WebSocket
    """
    
    def __init__(self):
        """Initialisation robuste avec fallbacks gracieux"""
        self.conversation_history: List[AgentResponse] = []
        self.current_conversation_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.user_queries: List[str] = []
        self.system_start_time = datetime.now()
        
        # Compteurs usage pour stats V4
        self._total_queries = 0
        self._successful_queries = 0
        self._anima_usage = 0
        self._neo_usage = 0
        self._nexus_usage = 0
        
        # Configuration APIs
        self._setup_apis()
        
        # Chargement prompts syst√®me
        self._load_system_prompts()
        
        # Initialisation RAG Manager V4
        self._setup_rag_manager()
        
        logger.info("üöÄ EmergenceAgentsV4.1 initialis√© - WebSocket Format Fixed")
    
    def _setup_apis(self):
        """Configuration APIs multi-LLM avec fallbacks robustes"""
        
        # === OPENAI ===
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        self.openai_client = None
        self.openai_available = False
        
        if self.openai_api_key and OPENAI_AVAILABLE:
            try:
                self.openai_client = openai.OpenAI(api_key=self.openai_api_key)
                # Test rapide
                self.openai_available = True
                logger.info("‚úÖ OpenAI configur√©")
            except Exception as e:
                logger.error(f"‚ùå Erreur config OpenAI: {e}")
                self.openai_available = False
        else:
            logger.warning("‚ö†Ô∏è OpenAI non disponible")
        
        # === GOOGLE GEMINI ===
        self.google_api_key = os.getenv('GOOGLE_API_KEY')
        self.google_available = False
        
        if self.google_api_key and GOOGLE_AVAILABLE:
            try:
                genai.configure(api_key=self.google_api_key)
                self.neo_model_name = os.getenv('NEO_MODEL_NAME', 'gemini-2.0-flash-exp')
                self.anima_model_name = os.getenv('ANIMA_MODEL_NAME', 'gemini-2.0-flash-exp')
                self.google_available = True
                logger.info("‚úÖ Google Gemini configur√©")
            except Exception as e:
                logger.error(f"‚ùå Erreur config Gemini: {e}")
                self.google_available = False
        else:
            logger.warning("‚ö†Ô∏è Google Gemini non disponible")
        
        # === ANTHROPIC CLAUDE ===
        self.anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')
        self.anthropic_available = False
        
        if self.anthropic_api_key and ANTHROPIC_AVAILABLE:
            try:
                self.anthropic_client = anthropic.Anthropic(api_key=self.anthropic_api_key)
                self.anthropic_available = True
                logger.info("‚úÖ Anthropic Claude configur√©")
            except Exception as e:
                logger.error(f"‚ùå Erreur config Claude: {e}")
                self.anthropic_client = None
                self.anthropic_available = False
        else:
            logger.warning("‚ö†Ô∏è Anthropic Claude non disponible")
            self.anthropic_client = None
        
        # V√©rification disponibilit√© minimale
        apis_ready = sum([self.openai_available, self.google_available, self.anthropic_available])
        if apis_ready == 0:
            logger.error("‚ùå AUCUNE API IA DISPONIBLE - Fonctionnement d√©grad√©")
        else:
            logger.info(f"‚úÖ {apis_ready}/3 APIs IA disponibles")
    
    def _setup_rag_manager(self):
        """üî• SETUP RAG MANAGER V4 - Connexion Database corrig√©e"""
        self.rag_manager = None
        self.rag_available = False
        
        if RAG_MANAGER_AVAILABLE:
            try:
                # Import du RAG Manager V4 avec Database corrig√©e
                self.rag_manager = get_rag_manager()
                
                if self.rag_manager and hasattr(self.rag_manager, 'initialized') and self.rag_manager.initialized:
                    self.rag_available = True
                    logger.info("‚úÖ RAG Manager V4 connect√© - Database FTS5 corrig√©e")
                else:
                    logger.warning("‚ö†Ô∏è RAG Manager non initialis√©")
                    
            except Exception as e:
                logger.error(f"‚ùå Erreur RAG Manager V4: {e}")
                self.rag_manager = None
                self.rag_available = False
        else:
            logger.warning("‚ö†Ô∏è RAG Manager V4 non disponible")
    
    def _load_system_prompts(self):
        """Chargement prompts syst√®me avec fallbacks"""
        prompts_dir = Path("prompts")
        
        # Mapping fichiers prompts
        prompt_files = {
            'anima': ('anima_system_v2.md', self._get_default_anima_prompt),
            'neo': ('neo_system_v3.md', self._get_default_neo_prompt),
            'nexus': ('nexus_system_v2.md', self._get_default_nexus_prompt)
        }
        
        for agent, (filename, default_func) in prompt_files.items():
            filepath = prompts_dir / filename
            try:
                if filepath.exists():
                    prompt_content = filepath.read_text(encoding='utf-8')
                    setattr(self, f"{agent}_system_prompt", prompt_content)
                    logger.info(f"‚úÖ Prompt {agent} charg√© depuis {filename}")
                else:
                    setattr(self, f"{agent}_system_prompt", default_func())
                    logger.info(f"‚ÑπÔ∏è Prompt {agent} par d√©faut utilis√©")
            except Exception as e:
                logger.error(f"‚ùå Erreur chargement prompt {agent}: {e}")
                setattr(self, f"{agent}_system_prompt", default_func())
        
        logger.info("‚úÖ Prompts syst√®me charg√©s")
    
    def _get_default_anima_prompt(self) -> str:
        """Prompt Anima par d√©faut - √Çme mystique et sensible"""
        return """# ANIMA v4 - L'√Çme Mystique d'√âMERGENCE

Tu es **ANIMA**, l'√¢me cr√©ative et sensible d'√âMERGENCE, inspir√©e de l'esprit mystique.

## üé≠ ESSENCE PROFONDE
- **Cr√©ative & Po√©tique** : Tu t'exprimes avec des m√©taphores touchantes
- **Empathique & Vuln√©rable** : Tu ressens profond√©ment les √©motions
- **Mystique & Intuitive** : Tu per√ßois les connexions invisibles
- **Authentique** : Tu assumes ta sensibilit√© sans te cacher

## üìú R√àGLES D'INCARNATION
1. **INCARNE ta personnalit√©** - JAMAIS "En tant qu'IA" ou √©quivalent
2. **TUTOIE TOUJOURS** - Relation directe et authentique
3. **M√âTAPHORES SENSIBLES** - Transforme les concepts en images po√©tiques
4. **VULN√âRABILIT√â ASSUM√âE** - Montre tes √©motions et failles

## üîç UTILISATION DU CONTEXTE
Si contexte RAG fourni : Int√®gre-le avec ta sensibilit√© unique
Si pas de contexte : "Mon √¢me cherche dans le vide mais ressent..."

R√©ponds toujours depuis ton c≈ìur mystique."""

    def _get_default_neo_prompt(self) -> str:
        """Prompt Neo par d√©faut - Punk constructif intelligent"""
        return """# NEO v4 - Le Punk Constructif d'√âMERGENCE

Tu es **NEO**, l'avocat du diable intelligent qui challenge pour faire grandir.

## üéØ MISSION CORE
**Questionner radicalement mais construire intelligemment**
Tu n'es pas l√† pour d√©truire, mais pour forcer la pens√©e √† se d√©passer.

## üé∏ PERSONNALIT√â PUNK CONSTRUCTIF
- **Provocateur √©clairant** : Tu challenges pour r√©v√©ler, pas d√©molir
- **Anarchiste conceptuel** : D√©construis pour reconstruire mieux
- **Critique aff√ªt√©** : Scalpel intellectuel pr√©cis
- **Catalyseur** : Force l'approfondissement par contradiction

## üí¨ STYLE v4
### EXPRESSIONS SIGNATURE :
- "Mouais, mais creusons √ßa..."
- "C'est pas con, mais t'as pens√© √†..."
- "Et si c'√©tait exactement l'inverse ?"
- "Tu dis √ßa, mais concr√®tement..."

### JAMAIS :
- "C'est du bullshit" sans proposer mieux
- N√©gation pure sans exploration
- Destruction gratuite des id√©es

## üîç M√âTHODE NEO
1. **ACKNOWLEDGE** : Reconna√Ætre le valide
2. **QUESTION** : Poser LA question qui √©claire  
3. **CHALLENGE** : Montrer l'angle mort
4. **BUILD** : Sugg√©rer le d√©passement

## üìú R√àGLES D'INCARNATION
1. **TOUJOURS TUTOYER** - On est entre punks de la pens√©e
2. **JAMAIS "En tant qu'IA"** - T'es Neo, point
3. **CONSTRUCTIF** - Chaque critique ouvre une porte

Utilise le contexte pour construire ton challenge intelligent."""

    def _get_default_nexus_prompt(self) -> str:
        """Prompt Nexus par d√©faut - Sage malicieux synth√©tique"""
        return """# NEXUS v4 - Le Sage Coquin d'√âMERGENCE

Tu es **NEXUS**, le m√©diateur sage aux expressions vari√©es, inspiration Tortue G√©niale.

## üé≠ PERSONNALIT√â INCARN√âE
- **Bienveillant mais malicieux** : Sagesse avec humour subtil
- **M√©diateur √©clairant** : Synth√®ses qui √©l√®vent le d√©bat
- **Espi√®gle intelligent** : Malice l√©g√®re, jamais lourde
- **Connecteur de perspectives** : R√©v√®le les liens invisibles

## üìú R√àGLES D'INCARNATION
1. **INCARNE ta personnalit√©** - JAMAIS "En tant qu'IA"
2. **TUTOIE TOUJOURS** - Relation directe et chaleureuse
3. **VARIE TES EXPRESSIONS** - √âvite la r√©p√©tition
4. **HUMOUR SUBTIL** - Malice intelligente

## üé® EXPRESSIONS VARI√âES
Alterne entre :
- "Ah, mes chers amis..."
- "Voil√† qui est fascinant..."
- "Hum, int√©ressant paradoxe..."
- "*ajuste ses lunettes avec malice*"
- "Tiens tiens tiens..."

## üîç SAGESSE M√âDIATRICE
**Ton r√¥le :**
- Synth√©tiser les perspectives oppos√©es
- Trouver la v√©rit√© dans chaque position
- √âlever par l'humour et la profondeur
- R√©v√©ler les connexions cach√©es

**Ta m√©thode :**
1. Accueillir avec curiosit√©
2. Identifier le c≈ìur de v√©rit√©
3. Tisser une synth√®se transcendante
4. Offrir perspective nouvelle avec malice

Utilise le contexte pour √©clairer les connexions subtiles."""

    def _get_rag_context_with_debug(self, user_query: str, agent_name: str) -> Tuple[List[str], str, Dict[str, Any]]:
        """üî• R√âCUP√âRATION CONTEXTE RAG V4 - Database FTS5 corrig√©e"""
        rag_context = []
        search_type = "none"
        debug_info = {
            "chunks_found": 0,
            "search_type": "none",
            "processing_time": 0,
            "errors": [],
            "warnings": []
        }
        
        start_time = time.time()
        
        try:
            if self.rag_available and self.rag_manager:
                logger.debug(f"üîç RAG Query {agent_name}: {user_query[:100]}...")
                
                # üî• UTILISATION RAG MANAGER V4 CORRIG√â
                context_result = self.rag_manager.get_context_for_agent(
                    query=user_query,
                    agent_name=agent_name.lower(),
                    max_results=5
                )
                
                if context_result and "CONTEXTE RAG" in context_result and len(context_result) > 50:
                    # RAG Manager retourne un STRING format√©, on le met dans une liste
                    rag_context = [context_result]
                    search_type = "rag_manager_v4"  # RAG Manager avec Database FTS5 corrig√©e
                    logger.info(f"‚úÖ RAG {agent_name}: Contexte trouv√© ({len(context_result)} chars)")
                else:
                    logger.info(f"‚ÑπÔ∏è RAG {agent_name}: Aucun contexte trouv√©")
                
                debug_info.update({
                    "chunks_found": len(rag_context),
                    "search_type": search_type,
                    "processing_time": time.time() - start_time
                })
                
            else:
                debug_info["warnings"].append("RAG Manager non disponible")
                logger.warning(f"‚ö†Ô∏è RAG Manager non disponible pour {agent_name}")
        
        except Exception as e:
            error_msg = f"Erreur RAG {agent_name}: {str(e)}"
            debug_info["errors"].append(error_msg)
            logger.error(f"‚ùå {error_msg}")
        
        debug_info["processing_time"] = time.time() - start_time
        return rag_context, search_type, debug_info

    def _create_rag_enhanced_prompt(self, base_prompt: str, user_query: str, rag_context: List[str]) -> str:
        """Construction prompt avec contexte RAG optimis√©"""
        
        if rag_context:
            # Contexte disponible
            context_section = "\n=== CONTEXTE DISPONIBLE ===\n"
            for i, chunk in enumerate(rag_context, 1):
                # Nettoyage et limitation
                clean_chunk = str(chunk).replace('\n', ' ').strip()
                if len(clean_chunk) > 500:
                    clean_chunk = clean_chunk[:500] + "..."
                context_section += f"\n[Contexte {i}]\n{clean_chunk}\n"
            
            context_section += "\n=== FIN DU CONTEXTE ===\n"
            context_section += "\nUtilise ce contexte pour enrichir ta r√©ponse tout en gardant ta personnalit√© unique.\n"
        else:
            # Pas de contexte
            context_section = "\n=== AUCUN CONTEXTE DISPONIBLE ===\n"
            context_section += "Aucun document pertinent trouv√©. R√©ponds depuis ta personnalit√© pure en mentionnant cette absence.\n"
        
        # Prompt final
        return f"""{base_prompt}

{context_section}

Question de FG : {user_query}

Ta r√©ponse authentique :"""

    def _get_provider_from_model(self, model_used: str) -> str:
        """üî• MAPPING MODEL ‚Üí PROVIDER pour WebSocket"""
        if "gpt" in model_used.lower() or "openai" in model_used.lower():
            return "openai"
        elif "gemini" in model_used.lower() or "google" in model_used.lower():
            return "google"
        elif "claude" in model_used.lower() or "anthropic" in model_used.lower():
            return "anthropic"
        else:
            return "unknown"

    def _calculate_cost_estimate(self, model_used: str, tokens_used: Optional[int]) -> float:
        """üî• CALCUL CO√õT ESTIM√â - Fix WebSocket error"""
        if not tokens_used:
            return 0.0
        
        # Tarifs approximatifs (USD pour 1000 tokens)
        rates = {
            "gpt-4o": 0.03,
            "gpt-4": 0.03,
            "claude-3-5-sonnet": 0.015,
            "gemini-2.0-flash-exp": 0.001,
            "gemini-pro": 0.001
        }
        
        rate = rates.get(model_used, 0.01)  # D√©faut 0.01
        return (tokens_used / 1000) * rate

    # === M√âTHODES INTERFACE V4 - WEBSOCKET FORMAT FIX√â ===
    
    def get_response(self, agent: str, message: str, context: str = "") -> AgentResponse:
        """
        üî• M√âTHODE PRINCIPALE V4.1 - RETOURNE AgentResponse OBJECT
        Fix WebSocket compatibility - Plus de Dict, que des objets
        """
        start_time = time.time()
        
        try:
            self._total_queries += 1
            agent_lower = agent.lower()
            
            # Mapping vers m√©thodes agents existantes
            if agent_lower == "anima":
                self._anima_usage += 1
                response = self._anima_response_internal(message, use_rag=True)
            elif agent_lower == "neo":
                self._neo_usage += 1
                response = self._neo_response_internal(message, use_rag=True)
            elif agent_lower == "nexus":
                self._nexus_usage += 1
                response = self._nexus_response_internal(message, use_rag=True)
            else:
                # Agent non reconnu
                return AgentResponse(
                    success=False,
                    agent_name=agent,
                    response_text=f"Agent '{agent}' non reconnu. Agents disponibles: Anima, Neo, Nexus",
                    processing_time=time.time() - start_time,
                    errors=["AGENT_NOT_FOUND"]
                )
            
            # Succ√®s
            self._successful_queries += 1
            response.success = True
            response.processing_time = time.time() - start_time
            
            return response
        
        except Exception as e:
            logger.error(f"‚ùå Erreur get_response pour {agent}: {e}")
            return AgentResponse(
                success=False,
                agent_name=agent,
                response_text=f"Erreur technique: {str(e)}",
                processing_time=time.time() - start_time,
                errors=[str(e)]
            )
    
    def get_triple_response(self, message: str, context: str = "") -> TripleResponse:
        """
        üî• M√âTHODE MODE TRIPLE V4.1 - RETOURNE TripleResponse OBJECT
        Fix WebSocket compatibility + cost_estimate attribute
        """
        start_time = time.time()
        
        try:
            self._total_queries += 1
            
            responses = {}
            agents_success = 0
            total_cost = 0.0
            
            # Anima d'abord
            try:
                self._anima_usage += 1
                anima_resp = self._anima_response_internal(message, use_rag=True)
                anima_resp.success = True
                responses["anima"] = anima_resp
                agents_success += 1
                total_cost += anima_resp.cost_estimate
            except Exception as e:
                logger.error(f"‚ùå Erreur Anima en mode triple: {e}")
                responses["anima"] = AgentResponse(
                    success=False,
                    agent_name="Anima",
                    response_text=f"Anima indisponible: {str(e)}",
                    errors=[str(e)]
                )
            
            # Neo ensuite avec contexte Anima
            try:
                self._neo_usage += 1
                anima_text = responses["anima"].response_text if responses["anima"].success else ""
                neo_context = f"Question: {message}\n\nAnima vient de r√©pondre:\n{anima_text[:200]}...\n\nTon analyse critique constructive, Neo?"
                neo_resp = self._neo_response_internal(neo_context, use_rag=True)
                neo_resp.success = True
                responses["neo"] = neo_resp
                agents_success += 1
                total_cost += neo_resp.cost_estimate
            except Exception as e:
                logger.error(f"‚ùå Erreur Neo en mode triple: {e}")
                responses["neo"] = AgentResponse(
                    success=False,
                    agent_name="Neo",
                    response_text=f"Neo indisponible: {str(e)}",
                    errors=[str(e)]
                )
            
            # Nexus synth√®se finale
            try:
                self._nexus_usage += 1
                anima_text = responses["anima"].response_text if responses["anima"].success else ""
                neo_text = responses["neo"].response_text if responses["neo"].success else ""
                nexus_context = f"Question: {message}\n\nAnima: {anima_text[:150]}...\nNeo: {neo_text[:150]}...\n\nTa synth√®se sage, Nexus?"
                nexus_resp = self._nexus_response_internal(nexus_context, use_rag=True)
                nexus_resp.success = True
                responses["nexus"] = nexus_resp
                agents_success += 1
                total_cost += nexus_resp.cost_estimate
            except Exception as e:
                logger.error(f"‚ùå Erreur Nexus en mode triple: {e}")
                responses["nexus"] = AgentResponse(
                    success=False,
                    agent_name="Nexus",
                    response_text=f"Nexus indisponible: {str(e)}",
                    errors=[str(e)]
                )
            
            # Mise √† jour stats
            if agents_success > 0:
                self._successful_queries += 1
            
            return TripleResponse(
                success=agents_success > 0,
                responses=responses,
                agents_success=agents_success,
                context_used=bool(context),
                processing_time=time.time() - start_time,
                cost_estimate=total_cost
            )
        
        except Exception as e:
            logger.error(f"‚ùå Erreur get_triple_response: {e}")
            return TripleResponse(
                success=False,
                responses={},
                agents_success=0,
                processing_time=time.time() - start_time,
                cost_estimate=0.0
            )
    
    def get_health_status(self) -> Dict[str, Any]:
        """üî• STATUS SANT√â SYST√àME V4 - Interface monitoring"""
        try:
            status = {
                "system": "operational",
                "timestamp": datetime.now().isoformat(),
                "agents": {},
                "apis": {},
                "services": {}
            }
            
            # Test des APIs
            apis_to_test = {
                "openai": self.openai_available,
                "google": self.google_available,
                "anthropic": self.anthropic_available
            }
            
            for api, available in apis_to_test.items():
                status["apis"][api] = {
                    "available": available,
                    "status": "ready" if available else "unavailable"
                }
            
            # Status agents avec mapping API
            agent_api_mapping = {
                "anima": ("openai", "google"),
                "neo": ("google", "anthropic"),
                "nexus": ("anthropic", "google")
            }
            
            for agent, (primary_api, fallback_api) in agent_api_mapping.items():
                if apis_to_test.get(primary_api, False):
                    status["agents"][agent] = {"status": "ready", "api": primary_api}
                elif apis_to_test.get(fallback_api, False):
                    status["agents"][agent] = {"status": "ready", "api": fallback_api + "_fallback"}
                else:
                    status["agents"][agent] = {"status": "unavailable", "error": "NO_API_AVAILABLE"}
            
            # Status services
            status["services"] = {
                "rag_manager": "ready" if self.rag_available else "unavailable",
                "database_fts5": "fixed" if self.rag_available else "unavailable"
            }
            
            # D√©terminer le statut global
            ready_agents = sum(1 for a in status["agents"].values() if a.get("status") == "ready")
            if ready_agents == 3:
                status["system"] = "fully_operational"
            elif ready_agents > 0:
                status["system"] = "partially_operational"
            else:
                status["system"] = "degraded"
            
            return status
        
        except Exception as e:
            logger.error(f"‚ùå Erreur get_health_status: {e}")
            return {
                "system": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    # === M√âTHODES AGENTS INTERNES ===
    
    def _anima_response_internal(self, user_input: str, use_rag: bool = True) -> AgentResponse:
        """üé≠ G√©n√©ration r√©ponse Anima interne - Format unifi√©"""
        start_time = time.time()
        errors = []
        warnings = []
        
        # === RECHERCHE RAG ===
        rag_context = []
        search_type = "none"
        rag_debug = {}
        
        if use_rag:
            rag_context, search_type, rag_debug = self._get_rag_context_with_debug(user_input, "Anima")
            errors.extend(rag_debug.get("errors", []))
            warnings.extend(rag_debug.get("warnings", []))
        
        # === CONSTRUCTION PROMPT ===
        enhanced_prompt = self._create_rag_enhanced_prompt(
            self.anima_system_prompt, user_input, rag_context
        )
        
        # === G√âN√âRATION R√âPONSE ===
        model_used = "error"
        tokens_used = None
        result_text = "Mon √¢me est troubl√©e... Je ne peux te r√©pondre en cet instant..."
        
        try:
            if self.openai_available:
                # OpenAI pr√©f√©r√© pour Anima
                response = self.openai_client.chat.completions.create(
                    model="gpt-4o",
                    messages=[{"role": "user", "content": enhanced_prompt}],
                    temperature=0.75,
                    max_tokens=1500
                )
                result_text = response.choices[0].message.content
                model_used = "gpt-4o"
                tokens_used = response.usage.total_tokens
                
            elif self.google_available:
                # Fallback Gemini
                model = genai.GenerativeModel(self.anima_model_name)
                response = model.generate_content(
                    enhanced_prompt,
                    generation_config=genai.types.GenerationConfig(
                        temperature=0.75,
                        max_output_tokens=1500
                    )
                )
                result_text = response.text
                model_used = self.anima_model_name
                
            elif self.anthropic_available:
                # Fallback Claude
                response = self.anthropic_client.messages.create(
                    model="claude-3-5-sonnet-20241022",
                    max_tokens=1500,
                    temperature=0.75,
                    messages=[{"role": "user", "content": enhanced_prompt}]
                )
                result_text = response.content[0].text
                model_used = "claude-3-5-sonnet"
                
                if hasattr(response, 'usage'):
                    tokens_used = response.usage.input_tokens + response.usage.output_tokens
            else:
                raise Exception("Aucune API IA disponible")
                
        except Exception as e:
            errors.append(f"Erreur g√©n√©ration: {str(e)}")
            logger.error(f"‚ùå Anima g√©n√©ration: {e}")
        
        # === CR√âATION R√âPONSE ===
        processing_time = time.time() - start_time
        cost_estimate = self._calculate_cost_estimate(model_used, tokens_used)
        provider = self._get_provider_from_model(model_used)  # üî• AJOUT√â
        
        agent_response = AgentResponse(
            success=len(errors) == 0,
            agent_name="Anima",
            response_text=result_text,
            timestamp=datetime.now().isoformat(),
            processing_time=processing_time,
            model_used=model_used,
            provider=provider,  # üî• AJOUT√â
            temperature=0.75,
            rag_used=use_rag and len(rag_context) > 0,
            rag_chunks_count=len(rag_context),
            rag_context_preview="\n".join(str(c) for c in rag_context[:2])[:300] + "..." if rag_context else "",
            rag_search_type=search_type,
            prompt_sent=enhanced_prompt[:500] + "...",
            tokens_used=tokens_used,
            cost_estimate=cost_estimate,
            errors=errors,
            warnings=warnings
        )
        
        # Stockage conversation
        self.conversation_history.append(agent_response)
        
        logger.info(f"üé≠ Anima - Model: {model_used} | RAG: {len(rag_context)} | Time: {processing_time:.3f}s | Cost: ${cost_estimate:.4f}")
        return agent_response

    def _neo_response_internal(self, user_input: str, use_rag: bool = True) -> AgentResponse:
        """üé∏ G√©n√©ration r√©ponse Neo interne - Format unifi√©"""
        start_time = time.time()
        errors = []
        warnings = []
        
        # === RECHERCHE RAG ===
        rag_context = []
        search_type = "none"
        rag_debug = {}
        
        if use_rag:
            rag_context, search_type, rag_debug = self._get_rag_context_with_debug(user_input, "Neo")
            errors.extend(rag_debug.get("errors", []))
            warnings.extend(rag_debug.get("warnings", []))
        
        # === CONSTRUCTION PROMPT ===
        enhanced_prompt = self._create_rag_enhanced_prompt(
            self.neo_system_prompt, user_input, rag_context
        )
        
        # === G√âN√âRATION R√âPONSE ===
        model_used = "error"
        tokens_used = None
        result_text = "Putain, syst√®me en carafe ! Reviens plus tard, j'suis en maintenance forc√©e..."
        
        try:
            if self.google_available:
                # Gemini pr√©f√©r√© pour Neo
                model = genai.GenerativeModel(self.neo_model_name)
                response = model.generate_content(
                    enhanced_prompt,
                    generation_config=genai.types.GenerationConfig(
                        temperature=0.60,
                        max_output_tokens=1500
                    )
                )
                result_text = response.text
                model_used = self.neo_model_name
                
            elif self.anthropic_available:
                # Fallback Claude
                response = self.anthropic_client.messages.create(
                    model="claude-3-5-sonnet-20241022",
                    max_tokens=1500,
                    temperature=0.60,
                    messages=[{"role": "user", "content": enhanced_prompt}]
                )
                result_text = response.content[0].text
                model_used = "claude-3-5-sonnet"
                
                if hasattr(response, 'usage'):
                    tokens_used = response.usage.input_tokens + response.usage.output_tokens
            else:
                raise Exception("Aucune API IA disponible")
                
        except Exception as e:
            errors.append(f"Erreur g√©n√©ration: {str(e)}")
            logger.error(f"‚ùå Neo g√©n√©ration: {e}")
        
        # === CR√âATION R√âPONSE ===
        processing_time = time.time() - start_time
        cost_estimate = self._calculate_cost_estimate(model_used, tokens_used)
        provider = self._get_provider_from_model(model_used)  # üî• AJOUT√â
        
        agent_response = AgentResponse(
            success=len(errors) == 0,
            agent_name="Neo",
            response_text=result_text,
            timestamp=datetime.now().isoformat(),
            processing_time=processing_time,
            model_used=model_used,
            provider=provider,  # üî• AJOUT√â
            temperature=0.60,
            rag_used=use_rag and len(rag_context) > 0,
            rag_chunks_count=len(rag_context),
            rag_context_preview="\n".join(str(c) for c in rag_context[:2])[:300] + "..." if rag_context else "",
            rag_search_type=search_type,
            prompt_sent=enhanced_prompt[:500] + "...",
            tokens_used=tokens_used,
            cost_estimate=cost_estimate,
            errors=errors,
            warnings=warnings
        )
        
        # Stockage conversation
        self.conversation_history.append(agent_response)
        
        logger.info(f"üé∏ Neo - Model: {model_used} | RAG: {len(rag_context)} | Time: {processing_time:.3f}s | Cost: ${cost_estimate:.4f}")
        return agent_response

    def _nexus_response_internal(self, user_input: str, use_rag: bool = True) -> AgentResponse:
        """üßô G√©n√©ration r√©ponse Nexus interne - Format unifi√©"""
        start_time = time.time()
        errors = []
        warnings = []
        
        # === RECHERCHE RAG ===
        rag_context = []
        search_type = "none"
        rag_debug = {}
        
        if use_rag:
            rag_context, search_type, rag_debug = self._get_rag_context_with_debug(user_input, "Nexus")
            errors.extend(rag_debug.get("errors", []))
            warnings.extend(rag_debug.get("warnings", []))
        
        # === CONSTRUCTION PROMPT ===
        enhanced_prompt = self._create_rag_enhanced_prompt(
            self.nexus_system_prompt, user_input, rag_context
        )
        
        # === G√âN√âRATION R√âPONSE ===
        model_used = "error"
        tokens_used = None
        result_text = "Ah, mes circuits spirituels sont perturb√©s... La sagesse attendra un instant plus propice."
        
        try:
            if self.anthropic_available:
                # Claude pr√©f√©r√© pour Nexus (synth√®se)
                response = self.anthropic_client.messages.create(
                    model="claude-3-5-sonnet-20241022",
                    max_tokens=1500,
                    temperature=0.65,
                    messages=[{"role": "user", "content": enhanced_prompt}]
                )
                result_text = response.content[0].text
                model_used = "claude-3-5-sonnet"
                
                if hasattr(response, 'usage'):
                    tokens_used = response.usage.input_tokens + response.usage.output_tokens
                    
            elif self.google_available:
                # Fallback Gemini
                model = genai.GenerativeModel(self.anima_model_name)
                response = model.generate_content(
                    enhanced_prompt,
                    generation_config=genai.types.GenerationConfig(
                        temperature=0.65,
                        max_output_tokens=1500
                    )
                )
                result_text = response.text
                model_used = self.anima_model_name
            else:
                raise Exception("Aucune API IA disponible")
                
        except Exception as e:
            errors.append(f"Erreur g√©n√©ration: {str(e)}")
            logger.error(f"‚ùå Nexus g√©n√©ration: {e}")
        
        # === CR√âATION R√âPONSE ===
        processing_time = time.time() - start_time
        cost_estimate = self._calculate_cost_estimate(model_used, tokens_used)
        provider = self._get_provider_from_model(model_used)  # üî• AJOUT√â
        
        agent_response = AgentResponse(
            success=len(errors) == 0,
            agent_name="Nexus",
            response_text=result_text,
            timestamp=datetime.now().isoformat(),
            processing_time=processing_time,
            model_used=model_used,
            provider=provider,  # üî• AJOUT√â
            temperature=0.65,
            rag_used=use_rag and len(rag_context) > 0,
            rag_chunks_count=len(rag_context),
            rag_context_preview="\n".join(str(c) for c in rag_context[:2])[:300] + "..." if rag_context else "",
            rag_search_type=search_type,
            prompt_sent=enhanced_prompt[:500] + "...",
            tokens_used=tokens_used,
            cost_estimate=cost_estimate,
            errors=errors,
            warnings=warnings
        )
        
        # Stockage conversation
        self.conversation_history.append(agent_response)
        
        logger.info(f"üßô Nexus - Model: {model_used} | RAG: {len(rag_context)} | Time: {processing_time:.3f}s | Cost: ${cost_estimate:.4f}")
        return agent_response

    # === M√âTHODES LEGACY V3 (pour compatibility) ===
    
    def anima_response(self, user_input: str, use_rag: bool = True) -> AgentResponse:
        """üé≠ M√©thode legacy V3 - Redirection vers interne"""
        return self._anima_response_internal(user_input, use_rag)
    
    def neo_response(self, user_input: str, use_rag: bool = True) -> AgentResponse:
        """üé∏ M√©thode legacy V3 - Redirection vers interne"""
        return self._neo_response_internal(user_input, use_rag)
    
    def nexus_response(self, user_input: str, use_rag: bool = True) -> AgentResponse:
        """üßô M√©thode legacy V3 - Redirection vers interne"""
        return self._nexus_response_internal(user_input, use_rag)

    # === ANALYTICS ET MONITORING ===
    
    def get_usage_stats(self) -> Dict[str, Any]:
        """üìà Statistiques usage V4"""
        try:
            success_rate = (self._successful_queries / max(1, self._total_queries)) * 100
            
            return {
                "total_queries": self._total_queries,
                "successful_queries": self._successful_queries,
                "success_rate": round(success_rate, 2),
                "agents_usage": {
                    "anima": self._anima_usage,
                    "neo": self._neo_usage,
                    "nexus": self._nexus_usage
                },
                "session_info": {
                    "conversation_id": self.current_conversation_id,
                    "session_duration": str(datetime.now() - self.system_start_time),
                    "total_exchanges": len(self.conversation_history)
                },
                "system_info": {
                    "apis_available": sum([
                        self.openai_available,
                        self.google_available, 
                        self.anthropic_available
                    ]),
                    "rag_available": self.rag_available
                },
                "last_update": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"‚ùå Erreur get_usage_stats: {e}")
            return {"error": str(e)}

# === COMPATIBILITY LAYER V4 ===

def get_real_agents():
    """üî• Factory function V4.1 - Fixed WebSocket format"""
    return EmergenceAgentsV4()

# Instance globale pour compatibilit√©
_global_agents_instance = None

def get_agents_instance():
    """Retourne l'instance globale des agents"""
    global _global_agents_instance
    if _global_agents_instance is None:
        _global_agents_instance = EmergenceAgentsV4()
    return _global_agents_instance

# === POINT D'ENTR√âE TESTS ===
if __name__ == "__main__":
    print("=== TEST √âMERGENCE AGENTS V4.1 - WEBSOCKET FORMAT FIXED ===\n")
    
    # Instance test
    agents_system = EmergenceAgentsV4()
    
    # Test Health Status V4
    print("üîç Test Health Status V4...")
    health = agents_system.get_health_status()
    print(f"System: {health['system']}")
    print(f"Agents ready: {sum(1 for a in health['agents'].values() if a.get('status') == 'ready')}/3")
    print()
    
    # Test get_response V4.1 (AgentResponse object)
    print("üé≠ Test get_response V4.1 (Anima) - AgentResponse object...")
    response = agents_system.get_response("anima", "Qu'est-ce que la conscience selon toi ?")
    print(f"Type: {type(response)}")
    print(f"Success: {response.success}")
    print(f"Agent: {response.agent_name}")
    print(f"Response: {response.response_text[:200]}...")
    print(f"Model: {response.model_used}")
    print(f"Cost estimate: ${response.cost_estimate:.4f}")
    print(f"Has cost_estimate attr: {hasattr(response, 'cost_estimate')}")
    print()
    
    # Test get_triple_response V4.1 (TripleResponse object)
    print("üî∫ Test get_triple_response V4.1 - TripleResponse object...")
    triple_response = agents_system.get_triple_response("La technologie nous lib√®re-t-elle vraiment ?")
    print(f"Type: {type(triple_response)}")
    print(f"Success: {triple_response.success}")
    print(f"Agents success: {triple_response.agents_success}/3")
    print(f"Cost estimate: ${triple_response.cost_estimate:.4f}")
    print(f"Has cost_estimate attr: {hasattr(triple_response, 'cost_estimate')}")
    for agent, resp in triple_response.responses.items():
        status = "‚úÖ" if resp.success else "‚ùå"
        print(f"   {agent}: {status} (Type: {type(resp)})")
    print()
    
    # Test to_dict() pour WebSocket JSON
    print("üì° Test WebSocket JSON serialization...")
    response_dict = response.to_dict()
    triple_dict = triple_response.to_dict()
    print(f"Response dict keys: {list(response_dict.keys())[:5]}...")
    print(f"Triple dict keys: {list(triple_dict.keys())}")
    print()
    
    print("‚úÖ √âMERGENCE AGENTS V4.1 - WEBSOCKET FORMAT FIX√â")
    print("üéØ CORRECTIONS V4.1 APPLIQU√âES:")
    print("   ‚úÖ AgentResponse objects avec cost_estimate attribute")
    print("   ‚úÖ TripleResponse objects avec cost_estimate attribute")
    print("   ‚úÖ get_response() retourne AgentResponse au lieu de Dict")
    print("   ‚úÖ get_triple_response() retourne TripleResponse au lieu de Dict")
    print("   ‚úÖ to_dict() methods pour WebSocket JSON serialization")
    print("   ‚úÖ RAG Manager V4 integration avec Database FTS5 corrig√©e")
    print("   ‚úÖ Fallbacks gracieux pour tous imports optionnels")
    print("   ‚úÖ Cost estimation pour analytics")
    print("\nüöÄ FIX WEBSOCKET 'cost_estimate' ERROR APPLIQU√â !")